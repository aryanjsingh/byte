# Gemini Thinking: Streaming Architecture Deep Dive

This document details the technical implementation of the real-time "Thinking" streaming feature.

## Core Streaming Concept
The system uses a **dual-channel streaming architecture**. Instead of waiting for the full reasoning process to complete, the system establishes a WebSocket pipe that pushes two distinct types of data chunks in real-time:
1.  **Thinking Chunks (`thinking`)**: The internal monologue and reasoning steps.
2.  **Answer Chunks (`answer`)**: The final addressed response to the user.

This architecture eliminates "dead air" (latency) by showing the user the AI's internal progress immediately, even for complex tasks that take several seconds to compute.

## The Streaming Protocol (WebSocket)

The communication happens over a stateful WebSocket connection (`ws://...`). The server pushes JSON frames as soon as they are generated by the model.

### Protocol Contract

| Message Type | Payload Structure | Purpose | Frontend Action |
| :--- | :--- | :--- | :--- |
| **`thinking`** | `{ "type": "thinking", "content": "..." }` | Internal thought process | Appends text to the collapsable "Thinking" block. |
| **`answer`** | `{ "type": "answer", "content": "..." }` | Final response text | Appends text to the main message body. |
| **`done`** | `{ "type": "done", "thread_id": "..." }` | Stream completion | Finalizes the message, persists to history, and unlocks input. |
| **`error`** | `{ "type": "error", "error": "..." }` | Failure signal | Stops streaming and renders an error state. |

## Backend Streaming Logic

**File**: `backend/server.py`

The backend leverages Python's `async generator` capabilities to yield chunks from the LLM and immediately forward them to the socket. This avoids buffering the entire response.

```python
# Simplified Logic from server.py handling the stream
async for chunk in thinking_wrapper.generate_with_thinking_stream(...):
    # Determine the type of chunk (thought vs answer)
    if chunk.type == "thinking":
        # 1. Stream reasoning immediately to the Thinking Block
        await websocket.send_json({
            "type": "thinking",
            "content": chunk.content
        })
    elif chunk.type == "answer":
         # 2. Stream final answer chunks to the Answer Block
        await websocket.send_json({
            "type": "answer",
            "content": chunk.content
        })
```

## Frontend Stream Consumption

**File**: `frontend/src/components/chat/ChatInterface.tsx`

The frontend uses a **split-state approach** to render the incoming stream. It maintains two separate string buffers that are updated live as packets arrive.

### React State Model
```typescript
// Two distinct states for two distinct UI areas
const [thinkingContent, setThinkingContent] = useState("");
const [streamingContent, setStreamingContent] = useState("");
```

### The Stream Handler
The `onmessage` event listener acts as a router, directing chunks to the correct state variable. This allows the UI to update the "Thinking" accordion and the "Answer" bubble independently and simultaneously.

```typescript
// Frontend WebSocket Handler Logic
websocket.onmessage = (event) => {
    const data = JSON.parse(event.data);

    if (data.type === 'thinking') {
        // 1. Append to Thinking Buffer
        // Triggers re-render of the <Reasoning> component
        setThinkingContent(prev => prev + data.content);
        
    } else if (data.type === 'answer') {
        // 2. Append to Answer Buffer
        // Triggers re-render of the <MessageBubble> component
        setStreamingContent(prev => prev + data.content);
    }
    
    // ... handle 'done' and 'error' states
};
```

## Visualizing the Stream Flow

1.  **Time 0.0s**: User sends "Help me".
    *   *State*: `thinkingContent: ""`, `streamingContent: ""`
2.  **Time 0.5s**: Backend yields `thinking` chunk: "Checking generic greeting schemas".
    *   *Action*: UI renders the "Thinking" block.
    *   *State*: `thinkingContent: "Checking generic gre"`, `streamingContent: ""`
3.  **Time 1.2s**: Backend yields `thinking` chunk: "User appears to be requesting...".
    *   *Action*: UI appends text to thinking block.
    *   *State*: `thinkingContent: "Checking generic greeting schemas... User appears..."`, `streamingContent: ""`
4.  **Time 2.0s**: Backend yields `answer` chunk: "Namaste!".
    *   *Action*: UI renders the Answer bubble below the thinking block.
    *   *State*: `thinkingContent: (unchanged)`, `streamingContent: "Namaste!"`
5.  **Time 2.1s**: Backend yields `answer` chunk: " How can I...".
    *   *Action*: UI appends text to Answer bubble.
    *   *State*: `thinkingContent: (unchanged)`, `streamingContent: "Namaste! How can I..."`

This granular control allows the user to follow the AI's logic (Thinking) without it cluttering the final output (Answer).
